{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhPLsM/4Vn/v4e72/EA5xS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LyubomirT/spam-classifier/blob/main/ML_Spam_Identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Identifier With AI\n",
        "\n",
        "This classifier is designed to identify spam messages using artificial intelligence. It is implemented in Python, primarily utilizing the PyTorch and scikit-learn libraries for machine learning and natural language processing tasks."
      ],
      "metadata": {
        "id": "hCQSkZ3CYNWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the necessary libraries for the spam classifier. `torch` and `torch.nn` are used for creating the neural network model. `Dataset` and `DataLoader` from `torch.utils.data` are used for creating a custom dataset and loading data in batches. `CountVectorizer` from `sklearn.feature_extraction.text` is used for converting text data into numerical vectors. The `csv` library is used for reading the spam dataset. `time` is used for training time calculation, while `re` and `sys` are here only to format the training log better."
      ],
      "metadata": {
        "id": "kmPiwGnhAzaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "OmYT-YDIyAyz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import csv\n",
        "import time\n",
        "import re, sys\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `SpamClassifier` class which is a subclass of `nn.Module`. This class represents the neural network model for the spam classifier. The model consists of two linear layers and uses sigmoid activation function."
      ],
      "metadata": {
        "id": "zR9LhzXaA3VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, 16)\n",
        "        self.linear2 = nn.Linear(16, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3PFmr_Og7yKR"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `SpamDataset` class which is a subclass of `Dataset`. This class represents the spam dataset. It reads the data from a CSV file and converts the text messages into numerical vectors using `CountVectorizer`. The labels are also converted into integers."
      ],
      "metadata": {
        "id": "fvY03AMhA7d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data = []\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        messages = []\n",
        "        labels = []\n",
        "        with open(csv_file, \"r\") as f:\n",
        "            csv_reader = csv.reader(f)\n",
        "            for row in csv_reader:\n",
        "                if len(row) == 2:\n",
        "                    label, message = row\n",
        "                    messages.append(message)\n",
        "                    labels.append(int(label == 'spam'))  # Convert label to integer\n",
        "\n",
        "        # Convert messages to vectors\n",
        "        message_vectors = self.vectorizer.fit_transform(messages).toarray()\n",
        "\n",
        "        for vector, label in zip(message_vectors, labels):\n",
        "            self.data.append((vector, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        message_vector, label = self.data[idx]\n",
        "        return torch.tensor(message_vector, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Tcvet5kn78ln"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `train` function which trains the model on the training data. It uses a specified optimizer and loss function. The training is done for a specified number of epochs."
      ],
      "metadata": {
        "id": "lXJGwMrlA-_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Reprinter:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text = ''\n",
        "\n",
        "    def clear_line(self):\n",
        "        \"\"\"Clears the line before printing the new text.\"\"\"\n",
        "        sys.stdout.write('\\033[F')  # Move cursor up one line\n",
        "        sys.stdout.write('\\r' + ' ' * len(self.text))\n",
        "\n",
        "    def __call__(self, text):\n",
        "        \"\"\"Prints `text` and clears the previous line.\"\"\"\n",
        "        self.clear_line()\n",
        "        print(text, end='', flush=True)\n",
        "        self.text = text\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_data, train_loader, optimizer, loss_fn, epochs):\n",
        "    reprint = Reprinter()\n",
        "    start_time = time.time()  # Record the start time of training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data).squeeze()  # Remove the extra dimension from output\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                reprint(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch, batch_idx * len(data), len(train_data), 100.0 * batch_idx / len(train_loader), loss.item()\n",
        "                ))\n",
        "\n",
        "    end_time = time.time()  # Record the end time of training\n",
        "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "    print(f\"\\nTraining took approximately {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "5Jyqr5x979Qu"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `predict` function which makes a prediction on a given message using the trained model. It also sets up and trains the model using the training data from “spam.csv”. The trained model is then saved to “spam_classifier.pt”."
      ],
      "metadata": {
        "id": "MPIr4Y0FBPaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Settings\n",
        "choice = \"Medium (10 MB)\" # @param [\"Small (469 KB)\", \"Medium (10 MB)\"]\n",
        "model_name = \"spam_classifier_medium\" # @param {type:\"string\"}\n",
        "# Define the predict function\n",
        "def predict(model, message):\n",
        "    # Convert the input message to a tensor\n",
        "    message_vector = torch.tensor(train_dataset.vectorizer.transform([message]).toarray(), dtype=torch.float32).to(device)\n",
        "\n",
        "    # Move the model to the appropriate device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Make the prediction\n",
        "    output = model(message_vector)\n",
        "    confidence = output.item() * 100.0\n",
        "\n",
        "    return confidence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  deviceName = \"GPU\"\n",
        "else:\n",
        "  deviceName = \"CPU\"\n",
        "print(\"Using\", deviceName)\n",
        "print(\"____________________________________________________________\")\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "if choice==\"Small (469 KB)\":\n",
        "  train_dataset = SpamDataset(\"spam.csv\")\n",
        "elif choice==\"Medium (10 MB)\":\n",
        "  train_dataset = SpamDataset(\"spam_20.csv\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "input_size = len(train_dataset.vectorizer.get_feature_names_out())\n",
        "model = SpamClassifier(input_size)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "train(model, train_dataset, train_loader, optimizer, loss_fn, epochs=100)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), f\"{model_name}.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "9U7yHBRx8BqE",
        "outputId": "b01aab63-235c-4233-8350-f80bfa789adf"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "____________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-02a91040d7c2>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpamClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         return np.asarray(\n\u001b[0;32m-> 1486\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         return np.asarray(\n\u001b[0;32m-> 1486\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try The Model Out\n",
        "\n",
        "Feel free to try the model out in the cell below! Simply enter your message in the text field on the right."
      ],
      "metadata": {
        "id": "wr8eygN4BT6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inference { form-width: \"50%\" }\n",
        "text_input = \"BUY AN IPHONE NOW\" # @param {type:\"string\"}\n",
        "accuracy = 1 # @param {type:\"slider\", min:1, max:13, step:1}\n",
        "model_select = \"spam_classifier_medium\" # @param [\"spam_classifier_medium\", \"spam_classifier_small\"]\n",
        "\n",
        "# Load the model\n",
        "if model_select == \"spam_classifier_small\":\n",
        "  train_dataset = SpamDataset(\"spam.csv\")\n",
        "elif model_select == \"spam_classifier_medium\":\n",
        "  train_dataset = SpamDataset(\"spam_20.csv\")\n",
        "input_size = len(train_dataset.vectorizer.get_feature_names_out())\n",
        "model = SpamClassifier(input_size)\n",
        "model.load_state_dict(torch.load(f\"{model_select}.pt\"))\n",
        "\n",
        "# Make a prediction\n",
        "message = text_input\n",
        "confidence = predict(model, message)\n",
        "confidence = round(confidence, accuracy)\n",
        "print(f\"Confidence rate: {confidence}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6zGpnIkyJb5",
        "outputId": "42c98777-ecba-4681-9522-c4bef5bba9c4"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence rate: 75.6%\n"
          ]
        }
      ]
    }
  ]
}